{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Incremental Inference Logging\n",
    "\n",
    "**Objective:** Load the black-box sentiment classifier, process a subset of the IMDB dataset, and log the model's prediction probabilities incrementally (token by token) for each review. These logged trajectories will be the observation sequences for training our HMM surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add src directory to Python path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.config import (\n",
    "    MODEL_NAME, DATASET_NAME, \n",
    "    NUM_TRAIN_SAMPLES, MAX_TOKENS, LOG_FILE_PATH, DEVICE\n",
    ")\n",
    "from src.data_utils import get_tokenizer, load_imdb_data, preprocess_data_for_inference_logging\n",
    "from src.black_box_model import BlackBoxSentimentClassifier, log_inference_trajectories\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Black-Box Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_model = BlackBoxSentimentClassifier(model_name=MODEL_NAME, device=DEVICE)\n",
    "tokenizer = bb_model.tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_raw = load_imdb_data(split='train', num_samples=NUM_TRAIN_SAMPLES, shuffle=True)\n",
    "print(f\"Loaded {len(imdb_train_raw)} raw training samples.\")\n",
    "\n",
    "processed_train_data = preprocess_data_for_inference_logging(imdb_train_raw, tokenizer)\n",
    "print(f\"Processed {len(processed_train_data)} samples for HMM training set generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Log Inference Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories = log_inference_trajectories(processed_train_data, bb_model, max_len=MAX_TOKENS)\n",
    "\n",
    "train_trajectories = [t for t in train_trajectories if t.shape[0] > 0]\n",
    "\n",
    "print(f\"Generated {len(train_trajectories)} trajectories for HMM training.\")\n",
    "if train_trajectories:\n",
    "    print(f\"Example trajectory 0 shape: {train_trajectories[0].shape}\")\n",
    "    print(f\"Example trajectory 0 first 3 steps:\\n{train_trajectories[0][:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Logged Trajectories\n",
    "\n",
    "We save the list of numpy arrays. `np.savez_compressed` is suitable for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "if train_trajectories:\n",
    "    np.savez_compressed(LOG_FILE_PATH, *train_trajectories) # Use * to save as separate arrays arr_0, arr_1, ...\n",
    "    print(f\"Saved {len(train_trajectories)} trajectories to {LOG_FILE_PATH}\")\n",
    "else:\n",
    "    print(\"No trajectories to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Save/Load (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_trajectories:\n",
    "    loaded_data = np.load(LOG_FILE_PATH, allow_pickle=True) # allow_pickle might be needed if arrays are objects\n",
    "    loaded_trajectories = [loaded_data[f'arr_{i}'] for i in range(len(loaded_data.files))]\n",
    "    print(f\"Loaded back {len(loaded_trajectories)} trajectories.\")\n",
    "    assert len(loaded_trajectories) == len(train_trajectories)\n",
    "    if loaded_trajectories:\n",
    "        assert np.allclose(loaded_trajectories[0], train_trajectories[0])\n",
    "        print(\"Verification successful.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
